{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9813d1",
   "metadata": {
    "id": "2a9813d1"
   },
   "source": [
    "# Assignment-4 (Perceptron, SVM, and Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d13c8e6",
   "metadata": {
    "id": "7d13c8e6"
   },
   "source": [
    "This part of the assignment shall require you to code the perceptron classifier from the ground up. The perceptron classifier takes uses the weighted sum of input features, uses a threshold to classify between two classes. Usually these classes are -1 and 1 while the threshold is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "45f0421a",
   "metadata": {
    "id": "45f0421a"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JmMLz83F5rja",
   "metadata": {
    "id": "JmMLz83F5rja"
   },
   "source": [
    "# 1. Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2IBreuJnmsvB",
   "metadata": {
    "id": "2IBreuJnmsvB"
   },
   "outputs": [],
   "source": [
    "# import the assigned dataset\n",
    "features = ['RI', 'Na','Mg', 'Al','Si','K','Ca','Ba']\n",
    "df = pd.read_csv('glass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "D6LXkVd1mtJh",
   "metadata": {
    "id": "D6LXkVd1mtJh"
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "#we only need to scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "LIwB8dmYq9iH",
   "metadata": {
    "id": "LIwB8dmYq9iH"
   },
   "outputs": [],
   "source": [
    "# Make a train-test split with 80% to training and 20% to testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df['targets'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2g0SCXkcmtML",
   "metadata": {
    "id": "2g0SCXkcmtML"
   },
   "outputs": [],
   "source": [
    "# Normalize numerical features and encode the categorical features (if any)\n",
    "scaler = StandardScaler() #\n",
    "\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.fit_transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936fab42",
   "metadata": {
    "id": "936fab42"
   },
   "source": [
    "As this is a classification problem, the target variables must be categorical. Using the feature and target variable information, preprocess the dataset to use given features.\n",
    "\n",
    "This step might require students to scale the features, one-hot encode categorical FEATURES (if any).\n",
    "\n",
    "\n",
    "\n",
    "For feature scaling and one-hot encoding, go through:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204d3db",
   "metadata": {
    "id": "a204d3db"
   },
   "source": [
    "# 2: Creating a perceptron model for the processed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0014b24",
   "metadata": {
    "id": "b0014b24"
   },
   "source": [
    "The perceptron.py file in the resources provides functions to code the perceptron model from scratch.\n",
    "\n",
    "Using the file as reference, write the functions:\n",
    "\n",
    "1. cross_validation_split\n",
    "2. accuracy_metric\n",
    "3. evaluate_algorithm\n",
    "4. predict\n",
    "5. train_weights\n",
    "6. perceptron\n",
    "\n",
    "This step is aimed at providing a comprehensive understanding of the internal functioning of a perceptron model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b7429d7c",
   "metadata": {
    "id": "b7429d7c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2, 2, 1, 2, 4, 5, 2, 2, 2, 2, 2, 2, 4, 2, 2, 6, 1, 2, 2, 4, 1, 2, 2, 2, 2, 4, 1, 4, 2, 1, 2, 2, 2, 1, 6, 5, 2, 5, 4, 4, 2, 1, 2, 1, 6, 1, 2, 1, 2, 2, 5, 2, 1, 1, 2, 2]\n",
      "[3, 4, 2, 1, 1, 2, 2, 2, 2, 2, 4, 2, 3, 1, 6, 2, 6, 2, 2, 4, 2, 2, 2, 1, 2, 2, 4, 2, 1, 5, 2, 5, 2, 5, 2, 2, 5, 6, 2, 2, 2, 1, 5, 2, 2, 2, 6, 4, 4, 1, 5, 2, 2, 2, 5, 4, 5]\n",
      "[2, 2, 2, 2, 2, 2, 4, 6, 1, 1, 2, 1, 2, 1, 1, 6, 1, 2, 2, 5, 1, 1, 4, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 1, 4, 2, 2, 1, 6, 2, 6, 1, 2, 4, 2, 6, 2, 5, 2, 1, 2, 2, 6, 4, 2, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[35.08771929824561, 38.59649122807017, 26.31578947368421]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for i, fold in enumerate(folds):\n",
    "        train_set = [f for j, f in enumerate(folds) if j != i]\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "    # Calculate the dot product of the input features and the weights\n",
    "    # for each class.\n",
    "    dot_products = [0.0 for i in range(len(weights))]\n",
    "    for i in range(len(row)-1):\n",
    "        for j in range(len(weights)):\n",
    "            dot_products[j] += weights[j][i] * row[i]\n",
    "\n",
    "    # Find the index of the maximum dot product, which corresponds\n",
    "    # to the predicted class.\n",
    "    prediction = dot_products.index(max(dot_products))\n",
    "    return prediction + 1\n",
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "    # Initialize weights for each class with zeros.\n",
    "    num_classes = len(set([row[-1] for row in train]))\n",
    "    num_features = len(train[0]) - 1\n",
    "    weights = [[0.0 for i in range(num_features)] for j in range(num_classes)]\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            prediction = predict(row, weights)\n",
    "            error = row[-1] - prediction\n",
    "            # Update the weights for the correct class, if the target value is within the valid range.\n",
    "            # Decrease the target value by 1 to get a valid index for the weights list.\n",
    "            if 1 <= row[-1] <= num_classes:\n",
    "                weights[int(row[-1]) - 1][0] = weights[int(row[-1]) - 1][0] + l_rate * error\n",
    "                for i in range(len(row)-1):\n",
    "                    # Update the weight for the correct class and feature index.\n",
    "                    # Decrease the target value by 1 to get a valid index for the weights list.\n",
    "                    weights[int(row[-1]) - 1][i] = weights[int(row[-1]) - 1][i] + l_rate * error * row[i]\n",
    "    return weights\n",
    "\n",
    "# Perceptron Algorithm With Stochastic Gradient Descent\n",
    "def perceptron(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    weights = train_weights(train, l_rate, n_epoch)\n",
    "    for row in test:\n",
    "        prediction = predict(row, weights)\n",
    "        predictions.append(prediction)\n",
    "    print(predictions)\n",
    "    return(predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_folds = 3\n",
    "l_rate = 0.0005\n",
    "n_epoch = 5000\n",
    "\n",
    "\n",
    "scores = evaluate_algorithm(np.concatenate((train_X, train_Y), axis=1), perceptron, n_folds, l_rate, n_epoch)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb75836",
   "metadata": {
    "id": "5eb75836"
   },
   "source": [
    "# (Bonus) Perceptron model with relaxation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751fbd6a",
   "metadata": {
    "id": "751fbd6a"
   },
   "source": [
    "Using Relaxation (the descent theorem), compare the performance of perceptron model with and without relaxation (refer class lectures, slides for details on relaxation).\n",
    "\n",
    "Make modifications to the loss function in the perceptron model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a9657",
   "metadata": {
    "id": "951a9657"
   },
   "outputs": [],
   "source": [
    "# code for relaxation for the perceptron model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b79cbb",
   "metadata": {
    "id": "95b79cbb"
   },
   "source": [
    "# 3: Batch size for perceptron model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d27d5b9",
   "metadata": {
    "id": "6d27d5b9"
   },
   "source": [
    "Experiment with different batch sizes in the perceptron model (eg: 1, 4, 8).\n",
    "\n",
    "Report (with figures) the difference in performance when using different batch sizes. Inferences without plots might not be awarded points.\n",
    "\n",
    "Report the accuracies for various combinations of batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8b124",
   "metadata": {
    "id": "65f8b124"
   },
   "outputs": [],
   "source": [
    "# code for step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572ca2f",
   "metadata": {
    "id": "6572ca2f"
   },
   "outputs": [],
   "source": [
    "# code for step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VBCOjJts5QvA",
   "metadata": {
    "id": "VBCOjJts5QvA"
   },
   "outputs": [],
   "source": [
    "# code for step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wSYUKCg8neAH",
   "metadata": {
    "id": "wSYUKCg8neAH"
   },
   "source": [
    "# 4. SVM's\n",
    "\n",
    "### **Note : You are allowed to use sklearn's SVC classifier for steps 4.1 through 4.3**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dESPXbsWuF26",
   "metadata": {
    "id": "dESPXbsWuF26"
   },
   "source": [
    "# 4.1 Linear SVM\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Step 1: Implement a linear SVM model to classify the data points. (Look into the 'kernel' parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "GPZt7zZCmtPK",
   "metadata": {
    "id": "GPZt7zZCmtPK"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# linear SVM\n",
    "svm = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wDG07yCYt1B0",
   "metadata": {
    "id": "wDG07yCYt1B0"
   },
   "source": [
    "# Step 2: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "xRGroMUFt1J5",
   "metadata": {
    "id": "xRGroMUFt1J5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training - linear SVM\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aXe9XNrkDM",
   "metadata": {
    "id": "c2aXe9XNrkDM"
   },
   "source": [
    "# Step 3: Predict for the test points using the model trained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "18yLtskurkMR",
   "metadata": {
    "id": "18yLtskurkMR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 2, 1, 5, 2, 5, 5, 1, 2, 1, 7, 2, 2, 1, 1, 2, 2, 2, 2,\n",
       "       7, 7, 1, 1, 1, 2, 2, 2, 7, 2, 2, 2, 1, 2, 1, 6, 2, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict - linear SVM\n",
    "y_pred = svm.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RoR12SgBpzja",
   "metadata": {
    "id": "RoR12SgBpzja"
   },
   "source": [
    "# 4.2 Kernel SVM - Polynomial kernel\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Implement a kernel SVM model with a polynomial kernel to classify the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "lyDOfuYXp7dG",
   "metadata": {
    "id": "lyDOfuYXp7dG"
   },
   "outputs": [],
   "source": [
    "# kernel SVM - polynomial kernel\n",
    "clf = SVC(kernel='poly', degree=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GpP79PDauc2p",
   "metadata": {
    "id": "GpP79PDauc2p"
   },
   "source": [
    "# Step 2: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "uS6lHWUhuc_X",
   "metadata": {
    "id": "uS6lHWUhuc_X"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(degree=30, kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(degree=30, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(degree=30, kernel='poly')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training - kernel SVM\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eGEDVYRr7hz",
   "metadata": {
    "id": "5eGEDVYRr7hz"
   },
   "source": [
    "# Step 3: Predict for the test points using the model trained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c2CS_FKsr7pF",
   "metadata": {
    "id": "c2CS_FKsr7pF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 2, 1, 2, 2, 2, 5, 1, 2, 1, 7, 2, 2, 1, 1, 1, 2, 1, 2,\n",
       "       7, 7, 1, 1, 1, 2, 2, 2, 7, 2, 2, 6, 1, 2, 1, 6, 2, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict - kernel SVM\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BsKcw44Vp7sd",
   "metadata": {
    "id": "BsKcw44Vp7sd"
   },
   "source": [
    "# 4.3 Kernel SVM - Gaussian kernel\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Implement a kernel SVM model with a gaussian (Radian Basis function) kernel to classify the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "4nLNaT5SqR58",
   "metadata": {
    "id": "4nLNaT5SqR58"
   },
   "outputs": [],
   "source": [
    "# kernel SVM - gaussian kernel\n",
    "clf = SVC(kernel='rbf', C=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QO82alm_unrk",
   "metadata": {
    "id": "QO82alm_unrk"
   },
   "source": [
    "# Step 2: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "XRlivJbkunys",
   "metadata": {
    "id": "XRlivJbkunys"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1000)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training - kernel SVM\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YCEodCuXr_oi",
   "metadata": {
    "id": "YCEodCuXr_oi"
   },
   "source": [
    "# Step 3: Predict for the test points using the model trained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "euFQqnXrr_vz",
   "metadata": {
    "id": "euFQqnXrr_vz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 2, 1, 5, 2, 5, 2, 1, 2, 1, 7, 2, 2, 1, 1, 2, 2, 2, 2,\n",
       "       7, 7, 1, 1, 1, 2, 2, 2, 7, 2, 2, 2, 1, 2, 1, 6, 2, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict - kernel SVM\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D4rej5nCsE3e",
   "metadata": {
    "id": "D4rej5nCsE3e"
   },
   "source": [
    "# 4.4 - Evaluation\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Take the results from each predict step under sections 4.1, 4.2 and 4.3. Consider accuracy as the evaulation metric. Print the accuracies for each of the 3 SVM models.\n",
    "\n",
    "Note: Do not use functions from sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "huEssd0Cs4Su",
   "metadata": {
    "id": "huEssd0Cs4Su"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6511627906976745"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_accuracy(predicted, actual):\n",
    "    assert len(predicted) == len(actual)\n",
    "    count = 0\n",
    "    for (pred, actual) in zip(predicted, actual):\n",
    "        if pred == actual:\n",
    "            count += 1\n",
    "    return count / len(predicted)\n",
    "\n",
    "calculate_accuracy(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "Mbb7piNhvmz7",
   "metadata": {
    "id": "Mbb7piNhvmz7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [242]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# space for any imports for the following steps\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#libraries used for neural networks\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# space for any imports for the following steps\n",
    "\n",
    "#libraries used for neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yspPihkdtYzW",
   "metadata": {
    "id": "yspPihkdtYzW"
   },
   "source": [
    "# 5. Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DORVXa96vcl9",
   "metadata": {
    "id": "DORVXa96vcl9"
   },
   "source": [
    "# 5.1 Single layer neural network\n",
    "\n",
    "You can use either PyTorch or Tensorflow for the implementation\n",
    "\n",
    "---\n",
    "\n",
    "# Step 1: Implement a single layer neural network to classify the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "Eekz2V1OvWq8",
   "metadata": {
    "id": "Eekz2V1OvWq8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [241]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#you don't specify what we can and cannot use from tensorflow, so I will just use keras\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define the model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m, input_dim\u001b[38;5;241m=\u001b[39mn_inputs, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "#you don't specify what we can and cannot use from tensorflow, so I will just use keras\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=n_inputs, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9V0qytGdvYTx",
   "metadata": {
    "id": "9V0qytGdvYTx"
   },
   "source": [
    "# Step 2: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y6EOwJ70vYaD",
   "metadata": {
    "id": "y6EOwJ70vYaD"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_X, train_y, epochs=n_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ozkKUCzvvY7b",
   "metadata": {
    "id": "ozkKUCzvvY7b"
   },
   "source": [
    "# Step 3: Predict for the test points using the model trained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "EbjD6UXgvZBs",
   "metadata": {
    "id": "EbjD6UXgvZBs"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [240]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions on new data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(test_X)\n\u001b[1;32m      4\u001b[0m predictions\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Make predictions on new data\n",
    "predictions = model.predict(test_X)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pT2B094yv4o_",
   "metadata": {
    "id": "pT2B094yv4o_"
   },
   "source": [
    "# 5.2 Multi - Layer neural network\n",
    "\n",
    "---\n",
    "\n",
    "# Step 1: Implement a multi - layer neural network to classify the data points\n",
    "\n",
    "Additional note: use methods to avoid overfitting appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1-IqXvAiv48P",
   "metadata": {
    "id": "1-IqXvAiv48P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "chE47ggUv5Cj",
   "metadata": {
    "id": "chE47ggUv5Cj"
   },
   "source": [
    "# Step 2: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qLudrV1Mv5LK",
   "metadata": {
    "id": "qLudrV1Mv5LK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "-bGzeSaUv5S8",
   "metadata": {
    "id": "-bGzeSaUv5S8"
   },
   "source": [
    "# Step 3: Predict for the test points using the model trained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kggDlhMtv5ad",
   "metadata": {
    "id": "kggDlhMtv5ad"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "XT1hpdNiwLlh",
   "metadata": {
    "id": "XT1hpdNiwLlh"
   },
   "source": [
    "# 5.3 Multi - Layer neural network\n",
    "\n",
    "---\n",
    "\n",
    "# Step 1: Implement a multi - layer neural network to classify the data points\n",
    "\n",
    "**Note :** This must have a different network architecture from the model under section 5.2\n",
    "\n",
    "**Additional note:** use methods to avoid overfitting appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ldNn2tNVwLsW",
   "metadata": {
    "id": "ldNn2tNVwLsW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Ntq2avYhwNL8",
   "metadata": {
    "id": "Ntq2avYhwNL8"
   },
   "source": [
    "# Step 2: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hu7oU-eVwNSK",
   "metadata": {
    "id": "hu7oU-eVwNSK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pZuLLFWGwNYw",
   "metadata": {
    "id": "pZuLLFWGwNYw"
   },
   "source": [
    "# Step 3: Predict for the test points using the model trained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aUlxFb-mwNfg",
   "metadata": {
    "id": "aUlxFb-mwNfg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "i6OBpYaWxxwG",
   "metadata": {
    "id": "i6OBpYaWxxwG"
   },
   "source": [
    "# 5.4 - Evaluation\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Take the results from each predict step under sections 5.1, 5.2 and 5.3. Consider accuracy as the evaulation metric. Print the accuracies for each of the 3 neural network architectures.\n",
    "\n",
    "Note: Do not use functions from sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ry0jxD2Oxwq8",
   "metadata": {
    "id": "ry0jxD2Oxwq8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2n2RmSlmworT",
   "metadata": {
    "id": "2n2RmSlmworT"
   },
   "source": [
    "# 6 - Paragraph questions\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Q1 : Briefly explain the methods you used to prevent overfitting for the models under section 5.2 and 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4NNoPoY2wpiA",
   "metadata": {
    "id": "4NNoPoY2wpiA"
   },
   "source": [
    "***space for Q1's answer***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bXgMyWtEwo4K",
   "metadata": {
    "id": "bXgMyWtEwo4K"
   },
   "source": [
    "# Q2: Compare the performances of models under sections 2, 3, 4 and 5 namely perceptron, descent procedure, SVM's and neural networks. List down few points on why you think certain models performed better than others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m-612AuiwqWP",
   "metadata": {
    "id": "m-612AuiwqWP"
   },
   "source": [
    "***space for Q2's answer***"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
